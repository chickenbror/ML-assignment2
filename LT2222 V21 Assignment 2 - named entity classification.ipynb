{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this assignment, you are going to build a classifier for named entities from the Groningen Meaning Bank corpus.  Named entity recognition (NER) takes noun phrases from a text and identifies whether they are persons, organizations, and so on.  You will be using the Groningen Meaning Bank named entity corpus available on mltgpu at `/scratch/lt2222-v21-resources/GMB_dataset.txt`.  In this version of the task, you will assume we know *that* something is a named entity, and instead use multi-class classification to identify its type.  So you will be doing named entity classification but *not* recognition.\n",
    "\n",
    "The data looks like this: \n",
    "\n",
    "```\n",
    "3996    182.0   Nicole  NNP     B-per\n",
    "3997    182.0   Ritchie NNP     I-per\n",
    "3998    182.0   is      VBZ     O\n",
    "3999    182.0   pregnant        JJ      O\n",
    "4000    182.0   .       .       O\n",
    "4001    183.0   Speaking        VBG     O\n",
    "4002    183.0   to      TO      O\n",
    "4003    183.0   ABC     NNP     B-org\n",
    "4004    183.0   News    NNP     I-org\n",
    "4005    183.0   interviewer     NN      O\n",
    "4006    183.0   Dianne  NNP     B-per\n",
    "4007    183.0   Sawyer  NNP     I-per\n",
    "4008    183.0   ,       ,       O\n",
    "4009    183.0   the     DT      O\n",
    "4010    183.0   25-year-old     JJ      O\n",
    "4011    183.0   co-star NN      O\n",
    "4012    183.0   of      IN      O\n",
    "4013    183.0   TV      NN      O\n",
    "4014    183.0   's      POS     O\n",
    "4015    183.0   The     DT      B-art\n",
    "4016    183.0   Simple  NNP     I-art\n",
    "4017    183.0   Life    NNP     I-art\n",
    "4018    183.0   said    VBD     O\n",
    "4019    183.0   she     PRP     O\n",
    "4020    183.0   is      VBZ     O\n",
    "4021    183.0   almost  RB      O\n",
    "4022    183.0   four    CD      O\n",
    "4023    183.0   months  NNS     O\n",
    "4024    183.0   along   IN      O\n",
    "4025    183.0   in      IN      O\n",
    "4026    183.0   her     PRP$    O\n",
    "4027    183.0   pregnancy       NN      O\n",
    "4028    183.0   .       .       O\n",
    "```\n",
    "\n",
    "The first column is the line number.  The second column is a sentence number (for some reason given as a float; ignore it).  The third column is the word.  The fourth column is a part of speech (POS) tag in Penn Treebank format.  The last column contains the named entity annotation. \n",
    "\n",
    "The annotation works like this.  Every `O` just means that the row does not represent a named entity.  `B-xyx` means the first word in a named entity with type `xyx`. `I-xyz` means the second and later words of an `xyz` entity, if there are any.  That means that every time there's a `B` or an `I`, there's a named entity.  \n",
    "\n",
    "The entity types in the corpus are `art`,\n",
    "`eve`,\n",
    "`geo`,\n",
    "`gpe`,\n",
    "`nat`,\n",
    "`org`,\n",
    "`per`,\n",
    "and `tim`\n",
    "\n",
    "Your task is the following.\n",
    "1. To preprocess the text (lowercase and lemmatize; punctuation can be preserved as it gets its own rows).\n",
    "2. To create instances from every from every identified named entity in the text with the type of the NE as the class, and a surrounding context of five words on either side as the features.  \n",
    "3. To generate vectors and split the instances into training and testing datasets at random.\n",
    "4. To train a support vector machine (via `sklearn.svm.LinearSVC`) for classifying the NERs.\n",
    "5. To evaluate the performance of the classifier.\n",
    "\n",
    "You will do this by modifying a separate file containing functions that will be called from this notebook as a module.  You can modify this notebook for testing purposes but please only submit the original.  You will document everything in Markdown in README.md and submit a GitHub repository URL.\n",
    "\n",
    "This assignment is due on **Tuesday, 2021 March 9 at 23:59**.  It has **25 points** and **7 bonus points**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import a2\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmbfile = open('GMB_dataset.txt', \"r\")\n",
    "# (If running on MLT server)\n",
    "# gmbfile = open('/scratch/lt2222-v21-resources/GMB_dataset.txt', \"r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - preprocessing (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See step 1 above.  The data is coming to you as an unused file handle object.  You can return the data in any indexable form you like.  You can also choose to remove infrequent or uninformative words to reduce the size of the feature space. (Document this in README.md.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentenceNr</th>\n",
       "      <th>word</th>\n",
       "      <th>POS</th>\n",
       "      <th>NEtag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>from</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>that</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>country</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2</td>\n",
       "      <td>family</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2</td>\n",
       "      <td>soldier</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2</td>\n",
       "      <td>kill</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2</td>\n",
       "      <td>in</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2</td>\n",
       "      <td>conflict</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2</td>\n",
       "      <td>join</td>\n",
       "      <td>VBD</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2</td>\n",
       "      <td>protester</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2</td>\n",
       "      <td>who</td>\n",
       "      <td>WP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2</td>\n",
       "      <td>carry</td>\n",
       "      <td>VBD</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2</td>\n",
       "      <td>banner</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2</td>\n",
       "      <td>with</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2</td>\n",
       "      <td>such</td>\n",
       "      <td>JJ</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2</td>\n",
       "      <td>slogan</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2</td>\n",
       "      <td>as</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sentenceNr       word  POS NEtag\n",
       "20           1       from   IN     O\n",
       "21           1       that   DT     O\n",
       "22           1    country   NN     O\n",
       "23           2     family  NNS     O\n",
       "24           2         of   IN     O\n",
       "25           2    soldier  NNS     O\n",
       "26           2       kill  VBN     O\n",
       "27           2         in   IN     O\n",
       "28           2        the   DT     O\n",
       "29           2   conflict   NN     O\n",
       "30           2       join  VBD     O\n",
       "31           2        the   DT     O\n",
       "32           2  protester  NNS     O\n",
       "33           2        who   WP     O\n",
       "34           2      carry  VBD     O\n",
       "35           2     banner  NNS     O\n",
       "36           2       with   IN     O\n",
       "37           2       such   JJ     O\n",
       "38           2     slogan  NNS     O\n",
       "39           2         as   IN     O"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputdata = a2.preprocess(gmbfile)\n",
    "gmbfile.close()\n",
    "inputdata[20:40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Creating instances (7 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do step 2 above.  You will create a collection of Instance objects.  Remember to consider the case where the NE is at the beginning of a sentence or at the end, or close to either (you can create a special start token for that).  You can also start counting from before the `B` end of the NE mention and after the last `I` of the NE mention. That means that the instances should include things before and after the named entity mention, but not the named entity text itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances = a2.create_instances(inputdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Class: gpe Features: ['<s1>', '<s2>', '<s3>', '<s4>', '<s5>', 'this', 'week', 'restart', 'part', 'of'],\n",
       " Class: geo Features: ['the', 'conversion', 'process', 'at', 'its', 'nuclear', 'plant', '</s1>', '</s2>', '</s3>'],\n",
       " Class: gpe Features: ['<s1>', '<s2>', '<s3>', '<s4>', '<s5>', 'official', 'say', 'they', 'expect', 'to'],\n",
       " Class: tim Features: ['sensitive', 'part', 'of', 'the', 'plant', 'after', 'an', 'iaea', 'surveillance', 'system'],\n",
       " Class: org Features: ['the', 'plant', 'wednesday', 'after', 'an', 'surveillance', 'system', 'begin', 'function', '</s1>'],\n",
       " Class: org Features: ['<s2>', '<s3>', '<s4>', '<s5>', 'The', 'surveillance', 'system', 'begin', 'function', '</s1>'],\n",
       " Class: gpe Features: ['<s5>', 'The', 'european', 'union', 'with', 'backing', 'have', 'threaten', 'to', 'refer'],\n",
       " Class: gpe Features: ['backing', 'have', 'threaten', 'to', 'refer', 'to', 'the', 'u.n.', 'security', 'council'],\n",
       " Class: org Features: ['to', 'refer', 'iran', 'to', 'the', 'to', 'the', 'u.n.', 'security', 'council'],\n",
       " Class: gpe Features: ['impose', 'sanction', 'if', 'it', 'find', 'have', 'violate', 'the', 'nuclear', 'non-proliferation'],\n",
       " Class: art Features: ['find', 'tehran', 'have', 'violate', 'the', 'have', 'violate', 'the', 'nuclear', 'non-proliferation'],\n",
       " Class: gpe Features: ['<s1>', '<s2>', '<s3>', '<s4>', '<s5>', \"'s\", 'new', 'president', 'mahmoud', 'ahmadinejad'],\n",
       " Class: per Features: ['<s4>', '<s5>', 'iran', \"'s\", 'new', \"'s\", 'new', 'president', 'mahmoud', 'ahmadinejad'],\n",
       " Class: tim Features: ['new', 'president', 'mahmoud', 'ahmadinejad', 'say', 'that', 'european', 'incentive', 'aim', 'at'],\n",
       " Class: gpe Features: ['mahmoud', 'ahmadinejad', 'say', 'tuesday', 'that', 'incentive', 'aim', 'at', 'persuade', 'iran'],\n",
       " Class: gpe Features: ['<s1>', '<s2>', '<s3>', '<s4>', '<s5>', \"'s\", 'new', 'president', 'mahmoud', 'ahmadinejad'],\n",
       " Class: gpe Features: ['be', 'an', 'insult', 'to', 'the', 'nation', '</s1>', '</s2>', '</s3>', '</s4>'],\n",
       " Class: gpe Features: ['<s2>', '<s3>', '<s4>', '<s5>', 'Two', 'and', 'four', 'nigerian', 'oil', 'worker'],\n",
       " Class: gpe Features: ['<s5>', 'Two', 'german', 'and', 'four', 'oil', 'worker', 'be', 'kidnap', 'by'],\n",
       " Class: geo Features: ['raid', 'on', 'a', 'boat', 'in', \"'s\", 'southern', 'oil-rich', 'delta', 'region']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instances[20:40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Creating the table and splitting (10 points)\n",
    "\n",
    "Here you're going to write the functions that create a data table with \"document\" vectors representing each instance and split the table into training and testing sets and random with an 80%/20% train/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NE_class</th>\n",
       "      <th>vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>gpe</td>\n",
       "      <td>[0.2918489898787147, -0.2228923629539286, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>geo</td>\n",
       "      <td>[0.15747443330560373, 0.14620880871082087, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>gpe</td>\n",
       "      <td>[0.400964755379122, -0.3588337570380066, -0.01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>tim</td>\n",
       "      <td>[0.04316814481269091, 0.012291577274904292, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>org</td>\n",
       "      <td>[0.06821698262936311, 0.04101560676950451, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>org</td>\n",
       "      <td>[0.24514571016546285, -0.12948057912298272, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>gpe</td>\n",
       "      <td>[0.09509039751821698, -0.06196937321963152, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>gpe</td>\n",
       "      <td>[0.06741957692589473, -0.014630341345356138, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>org</td>\n",
       "      <td>[0.0900105570171458, 0.007763167024603573, 0.2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>gpe</td>\n",
       "      <td>[0.04254628685586361, -0.020090041525577085, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>art</td>\n",
       "      <td>[0.06053796160480387, -0.023316811899646895, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>gpe</td>\n",
       "      <td>[0.31680001032655414, -0.27649882535304104, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>per</td>\n",
       "      <td>[0.1296441987305436, -0.0974201531023911, 0.04...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>tim</td>\n",
       "      <td>[0.0440467120482244, -0.03052825649056039, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>gpe</td>\n",
       "      <td>[0.033652376687047035, -0.025925054178576858, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>gpe</td>\n",
       "      <td>[0.31680001032655414, -0.27649882535304104, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>gpe</td>\n",
       "      <td>[0.290403084836049, 0.25206971521377064, 0.099...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>gpe</td>\n",
       "      <td>[0.19376536622303753, -0.15782466160331124, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>gpe</td>\n",
       "      <td>[0.08092262540338077, -0.040285662138331464, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>geo</td>\n",
       "      <td>[0.06069580266796787, 0.017918695454887328, 0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   NE_class                                             vector\n",
       "20      gpe  [0.2918489898787147, -0.2228923629539286, -0.0...\n",
       "21      geo  [0.15747443330560373, 0.14620880871082087, 0.0...\n",
       "22      gpe  [0.400964755379122, -0.3588337570380066, -0.01...\n",
       "23      tim  [0.04316814481269091, 0.012291577274904292, 0....\n",
       "24      org  [0.06821698262936311, 0.04101560676950451, 0.0...\n",
       "25      org  [0.24514571016546285, -0.12948057912298272, -0...\n",
       "26      gpe  [0.09509039751821698, -0.06196937321963152, 0....\n",
       "27      gpe  [0.06741957692589473, -0.014630341345356138, 0...\n",
       "28      org  [0.0900105570171458, 0.007763167024603573, 0.2...\n",
       "29      gpe  [0.04254628685586361, -0.020090041525577085, 0...\n",
       "30      art  [0.06053796160480387, -0.023316811899646895, 0...\n",
       "31      gpe  [0.31680001032655414, -0.27649882535304104, -0...\n",
       "32      per  [0.1296441987305436, -0.0974201531023911, 0.04...\n",
       "33      tim  [0.0440467120482244, -0.03052825649056039, 0.0...\n",
       "34      gpe  [0.033652376687047035, -0.025925054178576858, ...\n",
       "35      gpe  [0.31680001032655414, -0.27649882535304104, -0...\n",
       "36      gpe  [0.290403084836049, 0.25206971521377064, 0.099...\n",
       "37      gpe  [0.19376536622303753, -0.15782466160331124, -0...\n",
       "38      gpe  [0.08092262540338077, -0.040285662138331464, 0...\n",
       "39      geo  [0.06069580266796787, 0.017918695454887328, 0...."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigdf = a2.create_table(instances)\n",
    "bigdf[20:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.0900106 , -0.08598886,  0.12926705, ...,  0.0022298 ,\n",
       "          0.00179209,  0.00265109],\n",
       "        [ 0.2543241 ,  0.30026571, -0.04386027, ..., -0.00096746,\n",
       "          0.00088428, -0.00735862],\n",
       "        [ 0.0296561 ,  0.00051986,  0.07662743, ...,  0.02975004,\n",
       "         -0.01148424,  0.01464954],\n",
       "        ...,\n",
       "        [ 0.06889689,  0.01556479,  0.17568195, ..., -0.00245458,\n",
       "          0.00150949, -0.0067574 ],\n",
       "        [ 0.0564334 , -0.01593151,  0.10847073, ...,  0.00341775,\n",
       "          0.00203227, -0.00264359],\n",
       "        [ 0.20090732, -0.13070457,  0.10478341, ...,  0.00358857,\n",
       "          0.00185618,  0.00227532]]),\n",
       " 1019    per\n",
       " 4333    tim\n",
       " 6026    per\n",
       " 5466    org\n",
       " 5060    geo\n",
       "        ... \n",
       " 6781    tim\n",
       " 6833    per\n",
       " 370     org\n",
       " 2575    tim\n",
       " 2542    tim\n",
       " Name: NE_class, Length: 5538, dtype: object,\n",
       " array([[ 0.09170924,  0.02798102,  0.23018494, ..., -0.00436365,\n",
       "          0.0030632 ,  0.00391933],\n",
       "        [ 0.24271281, -0.17949593, -0.00728888, ..., -0.00121682,\n",
       "          0.00520029, -0.00835198],\n",
       "        [ 0.07892847,  0.03190286,  0.20048998, ..., -0.00272955,\n",
       "         -0.00125107,  0.00151132],\n",
       "        ...,\n",
       "        [ 0.29399428, -0.25499695, -0.05720782, ...,  0.00218013,\n",
       "         -0.00155895,  0.00733918],\n",
       "        [ 0.03218726,  0.01174036,  0.07798026, ...,  0.00554183,\n",
       "         -0.00222294, -0.00253826],\n",
       "        [ 0.01820246,  0.00142543,  0.03386503, ..., -0.00444141,\n",
       "         -0.00091063, -0.00233759]]),\n",
       " 5       geo\n",
       " 12      geo\n",
       " 18      tim\n",
       " 19      gpe\n",
       " 24      org\n",
       "        ... \n",
       " 6901    org\n",
       " 6908    geo\n",
       " 6909    geo\n",
       " 6913    tim\n",
       " 6915    per\n",
       " Name: NE_class, Length: 1384, dtype: object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X, train_y, test_X, test_y = a2.ttsplit(bigdf)\n",
    "\n",
    "# X and y mean feature matrix and class respectively.\n",
    "train_X, train_y, test_X, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1999422132331696"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_y) / (len(test_y) + len(train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1999422132331696"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_X) / (len(test_X) + len(train_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'geo'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_y[0] # *key error if 0th row not in test dada\n",
    "zeroth = test_y[0] if 0 in test_y else train_y[0]\n",
    "zeroth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 - Training the model (0 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part you won't do yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearSVC()\n",
    "model.fit(train_X, train_y)\n",
    "train_predictions = model.predict(train_X)\n",
    "test_predictions = model.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['tim', 'tim', 'per', ..., 'org', 'gpe', 'tim'], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1019    per\n",
       "4333    tim\n",
       "6026    per\n",
       "5466    org\n",
       "5060    geo\n",
       "       ... \n",
       "6781    tim\n",
       "6833    per\n",
       "370     org\n",
       "2575    tim\n",
       "2542    tim\n",
       "Name: NE_class, Length: 5538, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['org', 'per', 'geo', ..., 'gpe', 'per', 'per'], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5       geo\n",
       "12      geo\n",
       "18      tim\n",
       "19      gpe\n",
       "24      org\n",
       "       ... \n",
       "6901    org\n",
       "6908    geo\n",
       "6909    geo\n",
       "6913    tim\n",
       "6915    per\n",
       "Name: NE_class, Length: 1384, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5 - Evaluation (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigate for yourself what a \"confusion matrix\".  Then implement a function that takes the data and produces a confusion matrix in any readable form that allows us to compare the performance of the model by class.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5       geo\n",
       "12      geo\n",
       "18      tim\n",
       "19      gpe\n",
       "24      org\n",
       "       ... \n",
       "6901    org\n",
       "6908    geo\n",
       "6909    geo\n",
       "6913    tim\n",
       "6915    per\n",
       "Name: NE_class, Length: 1384, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y #1384 'gold' NE classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['org', 'per', 'geo', ..., 'gpe', 'per', 'per'], dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions #1384 preducted NE classes\n",
    "\n",
    "# confusion matrix: N*N ie 8*8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>art</th>\n",
       "      <th>eve</th>\n",
       "      <th>geo</th>\n",
       "      <th>gpe</th>\n",
       "      <th>nat</th>\n",
       "      <th>org</th>\n",
       "      <th>per</th>\n",
       "      <th>tim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>art</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eve</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>geo</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>232</td>\n",
       "      <td>76</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>45</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpe</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>78</td>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "      <td>45</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nat</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>org</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>79</td>\n",
       "      <td>32</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>per</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>81</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tim</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>16</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     art  eve  geo  gpe  nat  org  per  tim\n",
       "art    1    0    1    1    0    0    1    2\n",
       "eve    0    0    0    1    0    0    1    1\n",
       "geo    1    3  232   76    0   62   45   71\n",
       "gpe    1    0   50   78    1   34   45   17\n",
       "nat    0    0    1    0    2    0    0    0\n",
       "org    1    0   52   37    0   79   32   33\n",
       "per    0    1   24   47    0   39   81   19\n",
       "tim    2    0   55   18    1   29   16   90"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a2.confusion_matrix(test_y, test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>art</th>\n",
       "      <th>eve</th>\n",
       "      <th>geo</th>\n",
       "      <th>gpe</th>\n",
       "      <th>nat</th>\n",
       "      <th>org</th>\n",
       "      <th>per</th>\n",
       "      <th>tim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>art</th>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eve</th>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>geo</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1491</td>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "      <td>81</td>\n",
       "      <td>44</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpe</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>813</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>36</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nat</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>org</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>806</td>\n",
       "      <td>17</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>per</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>42</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>768</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tim</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>20</td>\n",
       "      <td>783</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     art  eve   geo  gpe  nat  org  per  tim\n",
       "art   44    0     0    0    0    0    1    0\n",
       "eve    0   36     1    0    0    0    0    0\n",
       "geo    1    1  1491   55    1   81   44   81\n",
       "gpe    0    0    41  813    0   38   36   18\n",
       "nat    0    0     0    0   15    0    0    0\n",
       "org    2    0    47   35    0  806   17   23\n",
       "per    0    3    42   52    0   46  768   22\n",
       "tim    0    1    33   17    0   23   20  783"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a2.confusion_matrix(train_y, train_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the matrix and describe your observations in README.md.  In particular, what do you notice about the predictions on the training data compared to those on the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Part A - Error analysis (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the weakest-performing classes in the confusion matrix (or any, if they all perform poorly to the same extent).  Find some examples in the test data on which the classifier classified incorrectly for those classes.  What do you think is the reason why those are hard?  Consider linguistic factors and statistical factors, if applicable.  Write your answer in README.md."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Part B - Expanding the feature space (7 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the entire process above, but incorporate part-of-speech tag information into the feature vectors.  It's your choice as to how to do this, but document it in README.md.  Your new process should run from the single call below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2.bonusb('GMB_dataset.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding each word:   0%|                                                                      | 0/66161 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'en_nlp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-ec1f3a3cb367>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mitems\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\t'\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# [indexNr, sentenceNr, Word, POS, NEtag]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mitems\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m  \u001b[1;31m# Turn sentenceNr \"NNN.0\" into integer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mitems\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemma_\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0men_nlp\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mitems\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Use Spacy to lemmatise & lowercase\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0mitems\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mitems\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'O'\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mitems\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0msentNr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPOS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNEclass\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'en_nlp' is not defined"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from tqdm import tqdm\n",
    "with open('GMB_dataset.txt', \"r\") as file:\n",
    "    # Initialize spacy 'en' model, keeping only tagger component needed for lemmatization\n",
    "    nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "    \n",
    "    sentences_dict = {}\n",
    "    for line in tqdm(file.readlines()[1:], desc='Adding each word: '):\n",
    "        line=line.strip('\\n')\n",
    "        items = line.split('\\t')  # [indexNr, sentenceNr, Word,\tPOS, NEtag]\n",
    "        items[1] = int( float(items[1]) )  # Turn sentenceNr \"NNN.0\" into integer\n",
    "        items[2] = ' '.join([w.lemma_ for w in en_nlp( items[2] )]).lower() # Use Spacy to lemmatise & lowercase\n",
    "        items[4] = False if items[4]=='O' else items[4]\n",
    "        sentNr, word, POS, NEclass=items[1:]\n",
    "        \n",
    "        if sentNr not in sentences_dict:\n",
    "            sentences_dict[sentNr]=[]\n",
    "        sentences_dict[sentNr].append( (word, POS, NEclass) )\n",
    "\n",
    "sentences_dict[189]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_dict[189]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEs_list=[]\n",
    "for nr, sent in sentences_dict.items():\n",
    "    for trio in sent:\n",
    "        word,POS,NEclass=trio[:]\n",
    "        if NEclass and NEclass[0]=='B':\n",
    "            words=[word]\n",
    "            i=sent.index(trio)\n",
    "            c=1\n",
    "            try:\n",
    "                \n",
    "                while sent[i+c][2]:\n",
    "                    words.append(sent[i+c][0])\n",
    "                    c+=1\n",
    "                    \n",
    "                if sent[i+c][2]==False:\n",
    "                    next5=[]\n",
    "                    for nxt in range(5):\n",
    "                        try:\n",
    "                            nextword=sent[i+c+nxt][0]\n",
    "                            next5.append(nextword)\n",
    "                        except IndexError:\n",
    "                            pass\n",
    "                        \n",
    "            except IndexError:\n",
    "                pass\n",
    "            \n",
    "            prev5=[]\n",
    "            for prev in range(5,0,-1):\n",
    "                try:\n",
    "                    if i-prev>=0:\n",
    "                        prevword=sent[i-prev][0]\n",
    "                        prev5.append(prevword)\n",
    "                except IndexError:\n",
    "                    pass\n",
    "                \n",
    "                \n",
    "            NE = ' '.join(words)\n",
    "            NEs_list.append((NE,NEclass[2:],prev5, next5))\n",
    "                \n",
    "            \n",
    "NEs_list  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(NEs_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
